# âœ‹ ASL_HandSpeak â€“ American Sign Language Recognition

**ASL_HandSpeak** is a real-time American Sign Language (ASL) recognition system built with **MediaPipe, OpenCV, and Machine Learning**.  
It can detect and classify ASL alphabet hand signs (Aâ€“Z, except J and Z due to motion requirements).  

This project is designed for learning, prototyping, and experimenting with **Computer Vision, Hand Tracking, and Gesture Recognition**.  
Users can collect their own dataset, train models, and run live ASL recognition using a webcam.  

---

## ğŸ“‚ Project Structure

```bash
ASL_HandSpeak/
â”‚â”€â”€ core/                     # Core modules for processing and utilities
â”‚   â”œâ”€â”€ draw_utils.py         # Drawing landmarks and skeleton visualization
â”‚   â”œâ”€â”€ hand_utils.py         # Hand detection & coordinate calculation helpers
â”‚   â”œâ”€â”€ logger.py             # Logging/exporting data into CSV
â”‚   â””â”€â”€ preprocess.py         # Data preprocessing before feeding into the model
â”‚
â”‚â”€â”€ model/                    # Trained models and dataset storage
â”‚   â”œâ”€â”€ asl_model.pkl         # Pre-trained ASL model (pickle format)
â”‚   â””â”€â”€ dataset/              # Dataset for training/testing
â”‚
â”‚â”€â”€ scripts/                  # Main scripts
â”‚   â”œâ”€â”€ train.py              # Train a new model
â”‚   â”œâ”€â”€ test_ASL.py           # Run real-time ASL recognition (webcam)
â”‚   â””â”€â”€ collect_dataset.py    # Collect hand sign data to build dataset
â”‚
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ .gitignore                # Ignored files for Git
â”‚â”€â”€ README.md                 # Project documentation
```
---

## âš™ï¸ Installation
# Clone repository
git clone https://github.com/DevNatapohn/ASL_HandSpeak.git
cd ASL_HandSpeak

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\Scripts\activate       # Windows

# Install dependencies
pip install -r requirements.txt

---

## ğŸš€ Usage
1. Run real-time ASL recognition  
python scripts/test_ASL.py  

2. Collect new dataset samples  
python scripts/collect_dataset.py  

3. Train a new model  
python scripts/train.py  

---

## ğŸ§  Model
Uses scikit-learn classifiers trained on MediaPipe 3D hand landmarks (x, y, z)  

Supports ASL alphabets Aâ€“Z (except J and Z)  

Extendable with your own dataset and retraining  

---

## ğŸ“¦ Requirements
- Python 3.8+  
- OpenCV  
- MediaPipe  
- NumPy  
- scikit-learn  
- pickle  

(Install all via `requirements.txt`)  
