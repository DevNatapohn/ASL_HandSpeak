# âœ‹ ASL_HandSpeak â€“ American Sign Language Recognition

**ASL_HandSpeak** is a real-time American Sign Language (ASL) recognition system built with **MediaPipe, OpenCV, and Machine Learning**.  
It can detect and classify ASL alphabet hand signs (Aâ€“Z, except J and Z due to motion requirements).  

This project is designed for learning, prototyping, and experimenting with **Computer Vision, Hand Tracking, and Gesture Recognition**.  
Users can collect their own dataset, train models, and run live ASL recognition using a webcam.  

---

## ğŸ“‚ Project Structure

```bash
ASL_HandSpeak/
â”œâ”€â”€ ğŸ“ data/ â€“ stores raw and processed datasets
â”‚ â”œâ”€â”€ asl_landmarks_xyz.csv â€“ raw landmarks (X, Y, Z) collected from Mediapipe
â”‚ â””â”€â”€ asl_features.csv â€“ extracted mathematical features used for training
â”‚
â”œâ”€â”€ ğŸ“ model/ â€“ trained machine learning models
â”‚ â””â”€â”€ asl_model.pkl â€“ RandomForest + Scaler model saved with pickle
â”‚
â”œâ”€â”€ ğŸ“ scripts/ â€“ main project scripts
â”‚ â”œâ”€â”€ collect_landmark_ASL.py â€“ collect hand landmarks from webcam and save to CSV
â”‚ â”œâ”€â”€ extract_features.py â€“ extract features (distances, angles, vectors, etc.) from landmarks
â”‚ â”œâ”€â”€ train_asl_model.py â€“ train RandomForest model and save as .pkl
â”‚ â””â”€â”€ test_ASL.py â€“ real-time webcam inference with prediction display
â”‚
â”œâ”€â”€ ğŸ“ sounds/ â€“ reserved for audio feedback/alerts
â”‚
â””â”€â”€ requirements.txt â€“ dependencies (OpenCV, Mediapipe, scikit-learn, etc.)
```
---

## âš™ï¸ Installation
# Clone repository
git clone https://github.com/DevNatapohn/ASL_HandSpeak.git
cd ASL_HandSpeak

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\Scripts\activate       # Windows

# Install dependencies
pip install -r requirements.txt

---

## ğŸš€ Usage
1. Run real-time ASL recognition  
python scripts/test_ASL.py  

2. Collect new dataset samples  
python scripts/collect_dataset.py  

3. Train a new model  
python scripts/train.py  

---

## ğŸ§  Model
Uses scikit-learn classifiers trained on MediaPipe 3D hand landmarks (x, y, z)  

Supports ASL alphabets Aâ€“Z (except J and Z)  

Extendable with your own dataset and retraining  

---

## ğŸ“¦ Requirements
- Python 3.8+  
- OpenCV  
- MediaPipe  
- NumPy  
- scikit-learn  
- pickle  

(Install all via `requirements.txt`)  
